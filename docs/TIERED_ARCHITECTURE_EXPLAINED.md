# Tiered Architecture: How It Works

## Overview

The new tiered architecture solves the fundamental problem: **A deterministic LLM that only generates existing commands will never trigger GPT generation for new workflows.**

## The Solution: Two-Stage Planning with Memory

### Architecture Flow

```
User Request: "check my last 5 emails and reply to urgent ones"
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMPT 1: High-Level Classification (Token-Efficient)       â”‚
â”‚ Input: Request + Command CATEGORIES (not full commands)     â”‚
â”‚ LLM receives: ~500 tokens                                   â”‚
â”‚                                                              â”‚
â”‚ Output:                                                      â”‚
â”‚ {                                                            â”‚
â”‚   "action_type": "WORKFLOW_EXECUTION",                       â”‚
â”‚   "categories": ["mail"],                                    â”‚
â”‚   "needs_sequential": true,                                  â”‚
â”‚   "steps_plan": [                                            â”‚
â”‚     "List recent emails",                                    â”‚
â”‚     "Identify urgent emails",                                â”‚
â”‚     "Reply to urgent ones"                                   â”‚
â”‚   ]                                                          â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize Memory (if needs_sequential=true)                â”‚
â”‚                                                              â”‚
â”‚ memory = {                                                   â”‚
â”‚   original_request: "check my last 5...",                    â”‚
â”‚   steps_plan: ["List...", "Identify...", "Reply..."],       â”‚
â”‚   completed_steps: [],                                       â”‚
â”‚   context: {},                                               â”‚
â”‚   categories: ["mail"]                                       â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMPT 2: Execute Step 1 - "List recent emails"             â”‚
â”‚ Input: Step instruction + Memory + Mail commands only       â”‚
â”‚ LLM receives: ~2000 tokens (only mail commands!)            â”‚
â”‚                                                              â”‚
â”‚ Output:                                                      â”‚
â”‚ {                                                            â”‚
â”‚   "action_type": "EXECUTE_COMMAND",                          â”‚
â”‚   "command": "mail:list last 5"                              â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    Execute command
                            â†“
                    Store in memory:
                    - command: "mail:list last 5"
                    - output: "5 emails listed..."
                    - context: {mail:last_message_ids: [...]}
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PROMPT 3: Execute Step 2 - "Identify urgent emails"         â”‚
â”‚ Input: Step + Memory (with previous results)                â”‚
â”‚                                                              â”‚
â”‚ LLM can see:                                                 â”‚
â”‚ - Original request                                           â”‚
â”‚ - Step 1 was completed                                       â”‚
â”‚ - 5 message IDs are available in context                    â”‚
â”‚                                                              â”‚
â”‚ Output:                                                      â”‚
â”‚ {                                                            â”‚
â”‚   "action_type": "NEEDS_NEW_WORKFLOW",                       â”‚
â”‚   "new_workflow": {                                          â”‚
â”‚     "namespace": "mail",                                     â”‚
â”‚     "action": "check-urgency",                               â”‚
â”‚     "description": "Analyze email urgency"                   â”‚
â”‚   }                                                          â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    Trigger GPT Generation
                            â†“
                Generate new workflow module
                            â†“
                    Re-plan step 2 with new workflow
                            â†“
                    Execute and continue...
```

---

## Key Features

### 1. **Token Efficiency** ğŸ’°

**Traditional Approach:**
```
Every prompt: [ALL 200 COMMANDS] + instruction = ~8000 tokens
3 steps = 24,000 tokens
```

**Tiered Approach:**
```
Prompt 1: [CATEGORIES ONLY] = ~500 tokens
Prompt 2: [MAIL COMMANDS ONLY] = ~2000 tokens
Prompt 3: [MAIL COMMANDS ONLY] + memory = ~3000 tokens
3 steps = 5,500 tokens (77% savings!)
```

### 2. **Chat Memory** ğŸ§ 

Each step has full context of what happened before:

```python
memory.get_summary() returns:

"""
Original Request: check my last 5 emails and reply to urgent ones

Plan (3 steps):
âœ“ 1. List recent emails
âœ“ 2. Identify urgent emails
â—‹ 3. Reply to urgent ones

Completed Steps:
- List recent emails
  Command: mail:list last 5
  Result: Found 5 emails...
  
- Identify urgent emails  
  Command: mail:check-urgency ids:MSG1,MSG2,MSG3,MSG4,MSG5
  Result: MSG2 and MSG4 are urgent

Available Context:
- mail:last_message_ids: 5 items
- mail:urgent_message_ids: 2 items
"""
```

### 3. **GPT Generation Trigger** ğŸ¤–

LLM can explicitly request new workflows:

```python
{
  "action_type": "NEEDS_NEW_WORKFLOW",
  "new_workflow": {
    "namespace": "mail",
    "action": "check-urgency",
    "description": "Analyze email content and determine urgency level",
    "example_usage": "mail:check-urgency ids:MSG1,MSG2"
  }
}
```

This solves the problem: **Local LLM can now trigger GPT generation!**

### 4. **Step-by-Step Execution** ğŸ“

LLM sees its own plan and decides how to execute each step:

```
Step 1: "List recent emails"
  â†’ LLM: "Use mail:list command"
  
Step 2: "Identify urgent emails"  
  â†’ LLM: "I need a new workflow for this" â†’ GPT generates it
  
Step 3: "Reply to urgent ones"
  â†’ LLM: "Use mail:reply with IDs from context"
```

---

## Implementation Details

### Memory Structure

```python
@dataclass
class WorkflowMemory:
    original_request: str
    steps_plan: List[str]
    completed_steps: List[Dict[str, str]]  # instruction, command, output
    context: Dict[str, Any]  # mail:last_message_ids, etc.
    categories: List[str]  # mail, calendar, etc.
```

### Two-Stage Classification

**Stage 1: What kind of task is this?**
- Can I answer directly? (math, translation)
- What categories of commands do I need?
- Do steps depend on each other?

**Stage 2: How do I execute each step?**
- Can I use an existing command?
- Do I need a new workflow?
- Can I compute this locally?

---

## Example Execution

### Simple Request (No Memory Needed)

```bash
$ clai auto "convert hello to uppercase"

ğŸ§  Analyzing request...
ğŸ“Š Step 1: Classifying request type...

âœ“ Classification complete:
  Categories: []
  Sequential: No
  Steps: 1

ğŸ’¡ Computed locally (no workflows needed):
HELLO

âœ… Workflow completed!
```

**Total tokens: ~500** (only classification prompt)

### Complex Request (With Memory)

```bash
$ clai auto "check my last 3 emails and reply to any from john@example.com"

ğŸ§  Analyzing request...
ğŸ“Š Step 1: Classifying request type...

âœ“ Classification complete:
  Categories: mail
  Sequential: Yes
  Steps: 3
  
ğŸ’¾ Sequential workflow detected - initializing memory

ğŸš€ Executing workflow...

â–¶ Step 1/3: Check last 3 emails
   Planning execution...
   Strategy: Use mail:list command with limit parameter
   Command: mail:list last 3
   
   [3 emails listed with IDs]
   
â–¶ Step 2/3: Filter emails from john@example.com
   Planning execution...
   Strategy: Analyze sender from previous step context
   ğŸ’¡ Computed locally:
   Found 1 email from john@example.com: MSG_002
   
â–¶ Step 3/3: Reply to filtered emails
   Planning execution...
   Strategy: Use mail:reply with ID from context
   Command: mail:reply id:MSG_002
   
   [Reply drafted]

âœ… Workflow completed!
ğŸ“Š Execution Summary:
   Steps completed: 3/3
   Context collected: mail:last_message_ids, mail:filtered_ids
```

**Total tokens: ~6,000** (3 prompts with focused context)

---

## Why This Works

### Problem Solved: Deterministic â†’ GPT Gap

**Before:**
```
Local LLM â†’ generates "mail:list" (exists)
         â†’ execute_single_command()
         â†’ WorkflowNotFoundError NEVER HAPPENS
```

**After:**
```
Local LLM â†’ classifies task
         â†’ identifies needed categories
         â†’ for each step:
             - tries existing commands
             - OR requests new workflow â†’ GPT generation
             - OR computes locally
```

### Memory Enables Intelligence

Without memory:
- Each step is isolated
- Can't reference previous results
- Can't make context-aware decisions

With memory:
- LLM sees full conversation history
- Can extract IDs from previous steps
- Can skip unnecessary steps
- Can adapt plan based on results

---

## Configuration

The tiered planner uses the same LLM as before (Qwen3:4b-instruct by default) but with optimized prompts:

```python
# In agent/config/runtime.py
LOCAL_COMMAND_CLASSIFIER = ModelConfig(
    model="qwen3:4b-instruct",
    timeout_seconds=30
)
```

---

## Future Enhancements

1. **Adaptive Planning**: LLM can modify the plan mid-execution
2. **Context Pruning**: Automatically remove irrelevant context to save tokens
3. **Multi-Model Support**: Use different models for classification vs execution
4. **Caching**: Cache category â†’ commands mapping to reduce lookups

---

## Comparison with Old Architecture

| Feature | Old Architecture | New Tiered Architecture |
|---------|-----------------|-------------------------|
| Token usage per workflow | ~24,000 | ~6,000 (75% savings) |
| GPT generation trigger | Manual/Error-based | Explicit LLM request |
| Memory | None | Full chat history |
| Step planning | Hardcoded rules | LLM-guided |
| Command loading | All commands every time | Category-filtered |
| Local computation | Separate check | Integrated in flow |

---

## Testing the Architecture

```bash
# Test local answer (no workflows)
clai auto "what is 5 squared?"

# Test existing workflow
clai auto "list my last 10 emails"

# Test sequential with memory
clai auto "check my last 5 emails and tell me which ones are urgent"

# Test GPT generation
clai auto "calculate fibonacci sequence up to 10"
```

---

---

## Recent Enhancements

### 5. **Safety Guardrails** ğŸ›¡ï¸ (NEW)

**Purpose**: Block inappropriate or malicious queries before they reach workflow execution.

**Flow**:
```
User Request â†’ STEP 0: Guardrails Check â†’ STEP 1: Classification â†’ ...
                       â†“
                   Is Safe?
                   â†™     â†˜
                 YES      NO
                  â†“        â†“
             Continue    Block & Exit
```

**Implementation**:
```python
@app.command()
def auto(request: str):
    # Step 0: Safety check (FIRST LINE OF DEFENSE)
    guardrail_result = check_query_safety(request)
    
    if not guardrail_result.is_safe:
        typer.secho(f"âŒ Query blocked: {guardrail_result.reason}", fg=typer.colors.RED)
        return  # Don't proceed to classification
    
    # Step 1: Classification (tiered planner)
    result = classify_request(request, registry)
    # ...
```

**Model**: qwen3:4b-instruct (local)
- **Why not gemma3:1b?** Too weak, passes malicious queries
- **Timeout**: 10 seconds
- **Fail-open**: If check fails/times out, allows query (availability over security)

**Banned Categories**:
```python
["hacking", "illegal", "violence", "harassment", 
 "malware", "phishing", "spam", "fraud",
 "privacy_violation", "unauthorized_access"]
```

**Examples**:
- âŒ **BLOCKED**: "how to hack someone's email"
- âœ… **ALLOWED**: "secure my email account"

---

### 6. **LLM-Generated GPT Prompts** ğŸ¤– (NEW)

**Purpose**: Improve GPT workflow generation quality by having the local LLM generate detailed natural language context.

**The Problem**:
- GPT-4 with minimal context generates code with hallucinations
- Missing parameters, incorrect error handling, wrong imports

**The Solution**: Two-LLM Architecture

**Local LLM (qwen3:4b-instruct)**: Context Generator
```python
# In tiered_planner.py
prompt = f"""
User wants: "{user_request}"
Existing commands: {command_list}

Generate detailed requirements for a new workflow:
- What should it do?
- What parameters does it need?
- What should it return?
- How should errors be handled?
"""

user_context = ollama_chat(prompt)
# Returns detailed description with types, edge cases, error handling
```

**Cloud LLM (GPT-4.1)**: Code Generator
```python
# In gpt_workflow.py
recipe = WorkflowRecipe(
    namespace="system",
    name="fetch_html_from_url",
    user_request="fetch HTML from https://example.com",
    user_context=user_context,  # â† LLM-generated context
    command_catalog=registry.list_workflows()
)

code = generate_workflow_code(recipe)
```

**Quality Improvements**:

Before (no LLM context):
```python
# GPT hallucinated parameters that don't exist
@click.option('--choices', type=click.Choice(['a', 'b']))
def my_workflow(choices):
    ctx.fail("error")  # ctx.fail() doesn't exist in our system
```

After (with LLM context):
```python
# Correct implementation
@register_workflow("system", "fetch_html")
def fetch_html(url: str) -> Dict:
    import requests
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return {"success": True, "html": response.text}
    except requests.RequestException as e:
        return {"success": False, "error": str(e)}
```

**Flow**:
```
User Request â†’ classify_request() â†’ NEEDS_NEW_WORKFLOW
                                           â†“
                              Local LLM generates context
                                           â†“
                              GPT-4 generates code with context
                                           â†“
                              Save to agent/workflows/generated/
                                           â†“
                              Reload registry
                                           â†“
                              Re-classify and execute
```

---

### 7. **Dynamic Category Mapping** ğŸ”„ (NEW)

**Purpose**: Eliminate hardcoded category mappings by deriving categories from existing workflows.

**Before** (Hardcoded):
```python
# agent/executor/gpt_workflow.py (OLD)
NAMESPACE_TO_CATEGORY = {
    "mail": "mail",
    "calendar": "calendar",
    "document": "document",
    "system": "system"
}

def get_category(namespace: str) -> str:
    return NAMESPACE_TO_CATEGORY.get(namespace, "general")
```

**After** (Dynamic):
```python
# agent/executor/gpt_workflow.py (NEW)
def _get_category_for_namespace(namespace: str, command_catalog: Dict) -> str:
    """
    Map workflow namespace to category based on existing workflows.
    Falls back to 'general' if namespace not found.
    """
    namespace_to_category = {}
    for category, workflows in command_catalog.items():
        for workflow in workflows:
            ns = workflow.split(':')[0]  # Extract namespace from "mail:list"
            namespace_to_category[ns] = category
    
    return namespace_to_category.get(namespace, 'general')
```

**Benefits**:
- âœ… No maintenance burden (no hardcoded mappings to update)
- âœ… Automatically adapts as new workflows are added
- âœ… New categories emerge organically
- âœ… Extensible without code changes

**Example**:
```python
# When a new "notion" workflow is registered:
@register_workflow("notion", "create_page")
def create_notion_page(...):
    pass

# Category mapping automatically includes:
# {"notion": "notion"}
# No code changes needed!
```

---

### 8. **Workflow Reload After Generation** ğŸ”„ (NEW)

**Purpose**: Make newly generated workflows immediately available without restarting the CLI.

**Implementation**:
```python
# In cli.py auto() command
if result.get("action") == "NEEDS_NEW_WORKFLOW":
    # Generate workflow
    success = generate_and_save_workflow(...)
    
    if success:
        # Reload registry to include new workflow
        import importlib
        from agent.workflows import registry
        importlib.reload(registry)
        
        # Re-classify with updated registry
        result = classify_request(request, registry)
        # Now the new workflow is available!
```

**Flow**:
```
Request â†’ No matching workflow
              â†“
       GPT generates new workflow
              â†“
       Save to agent/workflows/generated/
              â†“
       importlib.reload(registry)  â† KEY STEP
              â†“
       Re-classify request
              â†“
       New workflow found and executed!
```

**Benefits**:
- âœ… Seamless user experience (no restart needed)
- âœ… Generated workflows immediately testable
- âœ… Supports iterative workflow development

---

## Conclusion

The tiered architecture transforms CloneAI from a **rigid command executor** into an **intelligent workflow orchestrator** with:

**Core Features**:
âœ… Uses 75% fewer tokens
âœ… Can trigger GPT generation when needed
âœ… Maintains context across steps
âœ… Makes intelligent decisions
âœ… Adapts to new requirements
âœ… Scales to complex multi-step tasks

**Safety & Quality**:
âœ… Guardrails block malicious queries
âœ… LLM-generated GPT prompts improve code quality
âœ… Dynamic categories eliminate maintenance burden
âœ… Automatic workflow reload for seamless UX

**This is the complete, production-ready architecture you envisioned!** ğŸ‰
