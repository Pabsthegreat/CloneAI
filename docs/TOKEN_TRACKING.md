# Token Usage Tracking for Dynamic Workflow Generation

## Overview

The dynamic workflow generation system now displays detailed token usage information for every GPT API call. This helps you monitor costs and understand the resource consumption of automatically generated workflows.

## What You'll See

### 1. Attempt Number and Context
```
============================================================
ðŸ”„ Workflow Generation Attempt 1/2
Command: weather:forecast
============================================================
```

### 2. Token Usage Details (After Each GPT Call)
```
============================================================
ðŸ¤– GPT Workflow Generation - Token Usage
============================================================
Command: weather:forecast
Model: gpt-4.1
Input Tokens:  8,245
Output Tokens: 1,823
Total Tokens:  10,068
============================================================
```

### 3. Final Outcome Summary
**Success:**
```
============================================================
âœ… Workflow Generation SUCCESSFUL
============================================================
Command: weather:forecast
Total Attempts: 1
Module: weather_forecast.py
Summary: Fetches weather forecast for a specified city

Notes:
  â€¢ Uses external weather API
  â€¢ Handles multiple temperature units
  â€¢ Includes error handling for invalid cities
============================================================
```

**Failure (after max attempts):**
```
============================================================
âŒ Workflow Generation FAILED
============================================================
Command: database:backup
Total Attempts: 2
Errors encountered:
  1. Syntax error: invalid syntax at line 45
  2. Import error: module 'invalid_module' not found
============================================================
```

## Token Metrics Explained

### Input Tokens
- The number of tokens in the prompt sent to GPT
- Includes:
  - System instructions
  - Project structure
  - Sample workflows
  - Tool summaries
  - Command reference
  - Previous error feedback (on retries)
- **Cost**: ~$0.01 per 1,000 tokens (for gpt-4o/gpt-4.1)

### Output Tokens
- The number of tokens in GPT's response
- Includes:
  - Generated Python module code
  - Test code
  - Notes and summary
  - JSON formatting
- **Cost**: ~$0.03 per 1,000 tokens (for gpt-4o/gpt-4.1)

### Total Tokens
- Sum of input + output tokens
- Used for quick cost estimation
- Typical range: 8,000 - 15,000 tokens per generation

## Cost Estimation

### Pricing (as of October 2025)
| Model | Input Tokens | Output Tokens |
|-------|--------------|---------------|
| gpt-4o | $0.0025 / 1K | $0.010 / 1K |
| gpt-4-turbo | $0.010 / 1K | $0.030 / 1K |
| gpt-4.1 | $0.010 / 1K | $0.030 / 1K |

### Example Costs

**Single successful generation (10,000 tokens):**
- Input: 8,000 tokens Ã— $0.010 = $0.08
- Output: 2,000 tokens Ã— $0.030 = $0.06
- **Total**: ~$0.14

**Retry scenario (2 attempts, 20,000 total tokens):**
- Attempt 1: 9,000 tokens Ã— avg cost = ~$0.12
- Attempt 2: 11,000 tokens Ã— avg cost = ~$0.15
- **Total**: ~$0.27

**Daily usage (10 new workflows):**
- 10 workflows Ã— ~$0.15 = ~$1.50

## Retry Behavior

### When Does It Retry?
The system will retry (up to 2 attempts per command) if:
1. GPT API returns an error
2. Generated code has syntax errors
3. Generated code fails to compile
4. Generated code fails to import
5. Generated workflow fails during execution

### What Changes Between Attempts?
On retries, the input includes:
- Previous error messages
- Feedback on what went wrong
- Same context + additional guidance

This typically increases input tokens by 500-1,500 tokens.

### Example Retry Output
```
============================================================
ðŸ”„ Workflow Generation Attempt 2/2
Command: database:backup
âš ï¸  Retrying after 1 previous error(s)
============================================================

============================================================
ðŸ¤– GPT Workflow Generation - Token Usage
============================================================
Command: database:backup
Model: gpt-4.1
Input Tokens:  8,123  â† Increased from 7,892 due to error feedback
Output Tokens: 1,689
Total Tokens:  9,812
============================================================
```

## Configuration

### Maximum Attempts
Default: 2 attempts per command

Can be configured in `agent/executor/dynamic_workflow.py`:
```python
dynamic_manager = WorkflowGenerationManager(
    max_remote_calls_per_command=2  # Change this value
)
```

**Considerations:**
- Higher limits increase success rate but also cost
- Each attempt adds ~$0.10-$0.20
- Most commands succeed on first attempt (~70%)
- Complex commands may need 2 attempts (~25%)
- Some commands may never succeed (~5%)

### Model Selection
Default: `gpt-4.1`

Can be configured when initializing the generator:
```python
generator = GPTWorkflowGenerator(
    model="gpt-4o",  # Options: gpt-4o, gpt-4-turbo, gpt-4.1
    temperature=0.1,
    max_output_tokens=6000
)
```

**Model Recommendations:**
- **gpt-4o**: Best balance of cost and quality ($0.0025/1K input)
- **gpt-4-turbo**: Higher quality, more expensive ($0.010/1K input)
- **gpt-4.1**: Similar to gpt-4-turbo, good for complex workflows

## Monitoring Best Practices

### 1. Track Your Usage
Keep a log of token usage to monitor costs:
```bash
# Example daily log
2025-10-14: 5 workflows generated, 52,345 tokens, ~$0.70
2025-10-15: 3 workflows generated, 31,245 tokens, ~$0.42
2025-10-16: 8 workflows generated, 87,123 tokens, ~$1.16
```

### 2. Set Budget Alerts
In your OpenAI dashboard:
- Set monthly budget limits
- Enable email alerts at 50%, 75%, 90%
- Review usage weekly

### 3. Optimize for Cost
To reduce token usage:
- Use simpler command names (less context needed)
- Keep your codebase clean (smaller project tree)
- Use namespaces that match existing tools
- Avoid complex multi-step workflows

### 4. Review Generated Code
After successful generation:
- Review the generated module
- Check if it follows best practices
- Consider manual improvements
- This prevents future regenerations

## Troubleshooting

### "Token usage information not available"
If you see this warning, the OpenAI API response didn't include usage data. This is rare but can happen with:
- Very old API versions
- Certain model configurations
- API errors

**Solution**: The workflow will still work; you just won't see token counts.

### High Token Usage (>15,000 per attempt)
If you consistently see high token usage:
- Your project tree might be too large (check PROJECT_ROOT)
- Sample workflows are too verbose
- Consider reducing `char_limit` in `_read_file()` calls

**Default limits:**
- Project files: 4,000 chars each
- Registry source: 4,500 chars
- Input text: 110,000 chars total

### Token Limit Exceeded Errors
If GPT returns "token limit exceeded":
1. Input is too large (>128K tokens for most models)
2. Check if project structure is enormous
3. Consider using a model with larger context window

**Solutions:**
- Use gpt-4o (128K context)
- Reduce sample workflow sizes
- Simplify command descriptions

## Example Session

Here's what a typical workflow generation session looks like:

```bash
$ clai do "weather:forecast city:Seattle"
```

**Output:**
```
============================================================
ðŸ”„ Workflow Generation Attempt 1/2
Command: weather:forecast
============================================================

============================================================
ðŸ¤– GPT Workflow Generation - Token Usage
============================================================
Command: weather:forecast
Model: gpt-4.1
Input Tokens:  8,245
Output Tokens: 1,823
Total Tokens:  10,068
============================================================

============================================================
âœ… Workflow Generation SUCCESSFUL
============================================================
Command: weather:forecast
Total Attempts: 1
Module: weather_forecast.py
Summary: Fetches weather forecast for a specified city

Notes:
  â€¢ Uses external weather API
  â€¢ Handles multiple temperature units
  â€¢ Includes error handling for invalid cities
============================================================

Weather forecast for Seattle:
Temperature: 62Â°F (17Â°C)
Conditions: Partly cloudy
Humidity: 65%
Wind: 8 mph NW

ðŸ¤– Workflow generated automatically via GPT-4.1 integration.
```

**Cost for this command**: ~$0.14

## Advanced Features

### Store Parameter
The API call includes `store=True`, which:
- Saves conversation history on OpenAI's servers
- Enables future analysis and debugging
- No additional cost
- Can be disabled for privacy

### Metadata Tracking
Each API call includes metadata:
```python
metadata={
    "component": "workflow_generator",
    "command": "weather:forecast",
    "namespace": "weather",
}
```

This helps:
- Filter API logs in OpenAI dashboard
- Track which commands are most common
- Analyze generation patterns

### Deterministic Output
The system uses `seed=7` for more consistent results:
- Same input â†’ same output (mostly)
- Reduces unnecessary variations
- Helps with debugging
- Improves retry success rate

## FAQ

**Q: Why don't I see token usage?**
A: Make sure you have the latest version of the openai library (v2.3.0+) and that your API key is valid.

**Q: Can I disable token tracking?**
A: Yes, comment out the print statements in `agent/executor/gpt_workflow.py` (lines 119-132).

**Q: Does this add latency?**
A: No, token tracking is instant. The only delay is the GPT API call itself (5-15 seconds).

**Q: Are tokens counted before or after truncation?**
A: After truncation. The displayed counts reflect actual API usage.

**Q: What if I hit rate limits?**
A: OpenAI rate limits are separate from token usage. If you hit rate limits, wait and retry.

**Q: Can I get a detailed breakdown?**
A: Yes, check your OpenAI dashboard for detailed logs including timestamps and exact prompts.

---

**Last Updated**: October 14, 2025  
**Version**: 1.0  
**Status**: âœ… Active and tested
